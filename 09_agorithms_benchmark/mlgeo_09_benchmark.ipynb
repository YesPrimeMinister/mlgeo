{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Machine Learning in Geosciences ] \n",
    "Department of Applied Geoinformatics and Carthography, Charles University\n",
    "\n",
    "Lukas Brodsky lukas.brodsky@natur.cuni.cz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Algorithms Benchnark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark shall asure that you: \n",
    "\n",
    "* 1. Split training and test data; \n",
    "\n",
    "* 2. Use the identical train/test split(s) acros seach algorithm being tested; \n",
    "\n",
    "TODO:\n",
    "\n",
    "* 3. Do multiple train/test splits; \n",
    "\n",
    "* 4. Use more than five different datasets; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports for reading, visualizing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocessing data\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create synthetic data set\n",
    "X, y = make_classification(n_samples = 1000, n_features = 10, n_informative = 5,\n",
    "                            n_redundant = 5,  class_sep = 1.0,\n",
    "                            random_state = 42, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(X[(y==0),0], X[(y==0),1], 'b.')\n",
    "# plt.plot(X[(y==1),0], X[(y==1),1], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy array to pandas dataframe\n",
    "labels = [f\"Feature {ii+1}\" for ii in range(X.shape[1])]\n",
    "X = pd.DataFrame(X, columns = labels)\n",
    "y = pd.DataFrame(y, columns = [\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add more datasets \n",
    "# rng = np.random.RandomState(2)\n",
    "# X += 2 * rng.uniform(size=X.shape)\n",
    "# linearly_separable = (X, y)\n",
    "\n",
    "# X, y = make_moons(noise=0.5, random_state=42, n_samples = 1000)\n",
    "# ds1 = make_moons(noise=0.3, random_state=0)\n",
    "# ds2 = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "# ds3 = linearly_separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 20 % \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,\n",
    "                                                    random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "classifiers.update({\"DTC\": DecisionTreeClassifier()})\n",
    "classifiers.update({\"LSVC\": LinearSVC()})\n",
    "classifiers.update({\"SVC\": SVC()})\n",
    "classifiers.update({\"Random Forest\": RandomForestClassifier()})\n",
    "classifiers.update({\"AdaBoost\": AdaBoostClassifier()})\n",
    "classifiers.update({\"Extra Trees Ensemble\": ExtraTreesClassifier()})\n",
    "# classifiers.update({\"Gradient Boosting\": GradientBoostingClassifier()})\n",
    "# classifiers.update({\"MLP\": MLPClassifier()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of decision function labels\n",
    "DECISION_FUNCTIONS = {\"LSVC\", \"SVC\"}\n",
    "\n",
    "# Create dict for classifiers with feature_importances_ attribute\n",
    "FEATURE_IMPORTANCE = {\"Gradient Boosting\", \"Extra Trees Ensemble\", \"Random Forest\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate parameter grid\n",
    "parameters = {}\n",
    "\n",
    "\n",
    "# Update dict with Decision Tree Classifier\n",
    "parameters.update({\"DTC\": {\n",
    "                            \"classifier__max_depth\" : [1,2,3, 4, 5, 6, 7, 8],\n",
    "                            \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                            \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                             }})\n",
    "\n",
    "parameters.update({\"LSVC\": {\n",
    "                            \"classifier__C\": [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "                             }})\n",
    "\n",
    "parameters.update({\"SVC\": {\n",
    "                            \"classifier__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "                            \"classifier__gamma\": [\"auto\"],\n",
    "                            \"classifier__C\": [0.1, 0.5, 1, 5, 10, 50, 100],\n",
    "                            \"classifier__degree\": [1, 2, 3, 4, 5, 6]\n",
    "                             }})\n",
    "\n",
    "# Update dict with Random Forest Parameters\n",
    "parameters.update({\"Random Forest\": {\n",
    "                                    \"classifier__n_estimators\": [200],\n",
    "                                    \"classifier__max_depth\" : [3, 4, 5, 6, 7, 8],\n",
    "                                    \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                    \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                    \"classifier__n_jobs\": [-1]\n",
    "                                     }})\n",
    "\n",
    "\n",
    "# Update dict with Extra Trees\n",
    "parameters.update({\"Extra Trees Ensemble\": {\n",
    "                                            \"classifier__n_estimators\": [200],\n",
    "                                            \"classifier__max_depth\" : [3, 4, 5, 6, 7, 8],\n",
    "                                            \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                            \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                            \"classifier__n_jobs\": [-1]\n",
    "                                             }})\n",
    "\n",
    "# Update dict with AdaBoost\n",
    "parameters.update({\"AdaBoost\": {\n",
    "                                \"classifier__base_estimator\": [DecisionTreeClassifier(max_depth = ii) for ii in range(1,6)],\n",
    "                                \"classifier__n_estimators\": [200],\n",
    "                                \"classifier__learning_rate\": [0.001, 0.01, 0.05, 0.1, 0.25, 0.50, 0.75, 1.0]\n",
    "                                 }})\n",
    "\n",
    "# Update dict with Gradient Boosting\n",
    "# \"classifier__subsample\": [0.8, 0.9, 1]\n",
    "parameters.update({\"Gradient Boosting\": {\n",
    "                                        \"classifier__learning_rate\":[0.15,0.1,0.05,0.01,0.005,0.001],\n",
    "                                        \"classifier__n_estimators\": [200],\n",
    "                                        \"classifier__max_depth\": [2,3,4,5,6],\n",
    "                                        \"classifier__min_samples_split\": [0.005, 0.01, 0.05, 0.10],\n",
    "                                        \"classifier__min_samples_leaf\": [0.005, 0.01, 0.05, 0.10]                                        \n",
    "                                         }})\n",
    "\n",
    "# Update dict with MLPClassifier\n",
    "parameters.update({\"MLP\": {\n",
    "                            \"classifier__hidden_layer_sizes\": [(5), (10), (5,5), (10,10), (5,5,5), (10,10,10)],\n",
    "                            \"classifier__activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "                            \"classifier__learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "                            \"classifier__max_iter\": [100, 200, 300, 500, 1000, 2000],\n",
    "                            \"classifier__alpha\": list(10.0 ** -np.arange(1, 10)),\n",
    "                             }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Tuning and Evaluation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_label = 'Random Forest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifiers[classifier_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Tune and evaluate classifiers\n",
    "for classifier_label, classifier in classifiers.items():\n",
    "    # Print message to user\n",
    "    print(f\"Now tuning {classifier_label}.\")\n",
    "\n",
    "    # Scale features via Z-score normalization\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Define steps in pipeline\n",
    "    steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "\n",
    "    # Initialize Pipeline object\n",
    "    pipeline = Pipeline(steps = steps)\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = parameters[classifier_label]\n",
    "\n",
    "    # Initialize GridSearch object\n",
    "    gscv = GridSearchCV(pipeline, param_grid, cv = 5, n_jobs = -1, verbose = 1, scoring = \"roc_auc\")\n",
    "\n",
    "    # Fit gscv\n",
    "    gscv.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "    # Get best parameters and score\n",
    "    best_params = gscv.best_params_\n",
    "    best_score = gscv.best_score_\n",
    "\n",
    "    # Update classifier parameters and define new pipeline with tuned classifier\n",
    "    tuned_params = {item[12:]: best_params[item] for item in best_params}\n",
    "    # pass model parameters \n",
    "    classifier.set_params(**tuned_params)\n",
    "\n",
    "    # Make predictions\n",
    "    # if classifier_label in DECISION_FUNCTIONS:\n",
    "        # y_pred = gscv.decision_function(X_test)\n",
    "    # else:\n",
    "        # y_pred = gscv.predict_proba(X_test)[:, 1]\n",
    "    y_pred = gscv.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    # TODO add f1-score, et al. \n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    # f1  = metrics.f1_score(y_test, y_pred) \n",
    "    # acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    # rec = metrics.recall_score(y_test, y_pred)\n",
    "    # prc = metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "    # Save results\n",
    "    result = {\"Classifier\": gscv,\n",
    "              \"Best Parameters\": best_params,\n",
    "              \"Training AUC\": best_score,\n",
    "              \"Test AUC\": auc # , \n",
    "              # \"Test F1\": f1, \n",
    "              # \"Test Accuracy\": acc, \n",
    "              # \"Test Recall\": rec, \n",
    "              # \"Test Precision\": prc\n",
    "             }\n",
    "\n",
    "    results.update({classifier_label: result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualing Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = []\n",
    "test_auc = []\n",
    "index = []\n",
    "for classifier_label in results:\n",
    "    train_auc.append(results[classifier_label][\"Training AUC\"])\n",
    "    test_auc.append(results[classifier_label][\"Test AUC\"])\n",
    "    index.append(classifier_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_scores = pd.DataFrame({\n",
    "    \"Train\": train_auc, \n",
    "    \"Test\": test_auc  \n",
    "    }, \n",
    "    index=index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_scores.plot(kind=\"bar\", figsize=(15,7), ylim=(0.8,1.0))\n",
    "plt.title(\"Benchmark\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nav_menu": {
   "height": "252px",
   "width": "333px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
